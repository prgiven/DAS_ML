{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed095956",
   "metadata": {},
   "source": [
    "# DAS Microseismic Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6fe31a",
   "metadata": {},
   "source": [
    "### Written by Paige Given and Fantine Huot with Contributions from Ariel Lellouch, Bin Luo, Robert G. Clapp, Tamas Nemeth, Kurt Nihei, and Biondo L. Biondi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa4dbac",
   "metadata": {},
   "source": [
    "#### This notebook runs through the entire processing and prediction process for our ML workflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7453607e",
   "metadata": {},
   "source": [
    "## Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3a7b10",
   "metadata": {},
   "outputs": [],
   "source": [
    " cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eaeae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import math\n",
    "from obspy.signal.filter import bandpass\n",
    "import matplotlib.pyplot as plt\n",
    "import enum\n",
    "import os\n",
    "from typing import List, Sequence, Text, Tuple\n",
    "import logging\n",
    "import os\n",
    "import segyio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acbbf3f",
   "metadata": {},
   "source": [
    "## Check for visible GPU devices\n",
    "#### To maintain efficiency when working with large data, you will want to ensure you have a GPU connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c5ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c206eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1104f",
   "metadata": {},
   "source": [
    "## Set Datapath and Load in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd43f571",
   "metadata": {},
   "source": [
    "#### Set datapath and file pattern for unprocessed data that you want to run predictions on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f10afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/scratch/users/prgiven/'# replace me with datapath \n",
    "input_file_pattern = os.path.join(datapath, 'FORGE/', '*.sgy') # replace me with directory and file pattern           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec3a268",
   "metadata": {},
   "source": [
    "#### Find filename and load data from numpy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65511e18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filenames = sorted(tf.io.gfile.glob(input_file_pattern))\n",
    "print('Found {} files'.format(len(filenames)))\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af70109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with segyio.open(filenames[10], ignore_geometry=True) as f:\n",
    "    data = segyio.tools.collect(f.trace[:])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6bb8c9",
   "metadata": {},
   "source": [
    "##### Extract Time information from SEGY file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7f1d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "np_file_path = '/scratch/users/prgiven/FORGE/SILIXA_78A-32_iDASv3-P13_220423144614_fieldID000011.sgy'\n",
    "filename_time = os.path.basename(np_file_path)\n",
    "time_str = filename_time.split('_')[-2]\n",
    "start_time_str = f'20{time_str[:2]}-{time_str[2:4]}-{time_str[4:6]}_{time_str[6:8]}-{time_str[8:10]}-{time_str[10:12]}'\n",
    "\n",
    "# Convert the start time to a datetime object\n",
    "start_time = datetime.strptime(start_time_str, '%Y-%m-%d_%H-%M-%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_file_path = '/scratch/users/prgiven/FORGE2019_events/FORGE_78-32_iDASv3-P11_UTC190427172438.sgy'\n",
    "filename_time = os.path.basename(np_file_path)\n",
    "print(filename_time)\n",
    "time_str = filename_time.split('UTC')[-1]\n",
    "print(time_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f68063",
   "metadata": {},
   "source": [
    "#### Quality check by printing data and checking data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e8dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46309e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcf3caa",
   "metadata": {},
   "source": [
    "## Data pre-processing functions\n",
    "\n",
    "**bp filter**: band-pass filters the data.\\\n",
    "**run prediction**: runs the model prediction on the data\\\n",
    "**preprocess**: runs bandpass on data, reshapes to model input size, clips amplitude values, and converts to float 32 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82297c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3570f7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpfilter(data,dt,bp_low,bp_high):\n",
    "    sos = scipy.signal.iirfilter(N=15,Wn=[bp_low,bp_high],btype='bandpass',fs=1/dt,output='sos')\n",
    "    return np.float32(scipy.signal.sosfilt(sos,data,axis=-1))\n",
    "\n",
    "def hcfilter(data,dt,bp_high):\n",
    "    sos = scipy.signal.iirfilter(N=15,Wn=bp_high,btype='lowpass',fs=1/dt,output='sos')\n",
    "    return np.float32(scipy.signal.sosfilt(sos,data,axis=-1))\n",
    "\n",
    "def normalize(data, axis=-1):\n",
    "    stddev = np.std(data, axis=axis, keepdims=True)\n",
    "    return np.divide(data, stddev, out=np.zeros_like(data), where=stddev != 0)\n",
    "\n",
    "def decimate(data,axis=-1):\n",
    "    data=signal.decimate(data,4,axis=axis)\n",
    "    return np.float32(data)\n",
    "\n",
    "def med(data,axis=0):\n",
    "    data = data - np.median(data, axis=axis)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98929f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(inputs, model):\n",
    "    return model.predict(inputs, use_multiprocessing=True)\n",
    "\n",
    "def preprocess(data):\n",
    "    # crop it to input_height and width (cropping at the center)\n",
    "    data=med(data,axis=0)\n",
    "    data=hcfilter(data,0.002,100)\n",
    "    data=normalize(data,axis=-1)\n",
    "    data = data.reshape((1, 400, 250, 1)) # model input shape\n",
    "    data = np.clip(data, -10, 10) # stats\n",
    "    return np.float32(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9a6916",
   "metadata": {},
   "source": [
    "### Look at Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5830035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [20,10]\n",
    "plt.ylabel('Depth (m)')\n",
    "plt.xlabel('time (ms)')\n",
    "plt.title('Raw Window - Full')\n",
    "v=np.percentile(data,99)\n",
    "plt.imshow((data),cmap='seismic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a7414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess2(data):\n",
    "    # crop it to input_height and width (cropping at the center)\n",
    "    data=med(data,axis=0)\n",
    "    data=hcfilter(data,0.002,100)\n",
    "    data=normalize(data,axis=-1)\n",
    "    data = np.clip(data, -10, 10) # stats\n",
    "    return np.float32(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8297c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove top surface noise\n",
    "data_cut=data[200:,:]\n",
    "data_proccess=preprocess2(data_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7de2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [20,10]\n",
    "plt.ylabel('Depth (m)')\n",
    "plt.xlabel('time (ms)')\n",
    "plt.title('Processed Data')\n",
    "plt.imshow((data_proccess),cmap='gray',vmin=-3, vmax=3)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.ylim([1000,100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349595c7",
   "metadata": {},
   "source": [
    "## Load ML Model and Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab48177",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = 'train_20210525_142709_cnn2d_modular_dec_baseline_' # Replace me\n",
    "datapath2='/home/users/prgiven/DAS_ML/microseismic-detection-ml'\n",
    "ckpt = '{}/models/{}/ckpt'.format(datapath2, job_id)\n",
    "model = tf.keras.models.load_model(ckpt)\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15f1dcf",
   "metadata": {},
   "source": [
    "## Implement Sliding Windows, Pre-Procesing, and Running Prediction on Data\n",
    "\n",
    "The cell below will create sliding windows from continuous data with window size (400,250). It the pre-processes the data, and runs the prediction on the processed windows. The final output is in the form [inputted,logits] where the inputted is the inputted window, and the logits is the result of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9aebf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sliding_window(data,start_time,time_step_size,distance_location,stride):\n",
    "    n1=data.shape[0]\n",
    "    w1=400\n",
    "    d1=100 \n",
    "    n2=data.shape[1]\n",
    "    w2=250\n",
    "    d2=245 \n",
    "\n",
    "    num_windows1=len(range(0,n1-w1+1,d1))\n",
    "\n",
    "    num_windows2=len(range(0,n2-w2+1,d2))\n",
    "\n",
    "    logits=np.zeros((len(range(0,n1-w1+1,d1)),len(range(0,n2-w2+1,d2))))\n",
    "    window_start_time1=[] \n",
    "    window_distance_location1=[]\n",
    "    inputted=np.zeros((num_windows1,num_windows2,400,250))\n",
    "    count1=0\n",
    "    for i1 in range(0,n1-w1+1,d1):\n",
    "        count2=0\n",
    "\n",
    "        for i2 in range(0,n2-w2+1,d2):\n",
    "\n",
    "            data_new=preprocess(data[i1:i1+w1,i2:i2+w2])\n",
    "            window_start_time = start_time + timedelta(seconds=i2*time_step_size) + timedelta(milliseconds=i2*time_step_size*1000)\n",
    "            window_start_time1 = np.append(window_start_time1,window_start_time) \n",
    "\n",
    "            window_distance_location = distance_location[i1 // stride[0]]\n",
    "            window_distance_location1= np.append(window_distance_location1,window_distance_location)\n",
    "\n",
    "            data_new.shape\n",
    "\n",
    "            output = run_prediction(data_new, model)\n",
    "\n",
    "            logits[count1,count2] = output \n",
    "\n",
    "            inputted[count1,count2]=np.squeeze(data_new)\n",
    "            count2+=1\n",
    "        count1+=1\n",
    "    return inputted,logits,window_start_time1,window_distance_location1\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "probability_loop=[]\n",
    "time_loop=[]\n",
    "ap_loop=[]\n",
    "for i in range(0,len(filenames)):\n",
    "    with segyio.open(filenames[i], ignore_geometry=True) as f:\n",
    "        data = segyio.tools.collect(f.trace[:])\n",
    "        print(np.shape(data))\n",
    "        \n",
    "    #TIME INFO\n",
    "    from datetime import datetime, timedelta\n",
    "    np_file_path = filenames[i]\n",
    "    filename_time = os.path.basename(np_file_path)\n",
    "    time_str = filename_time.split('UTC')[-1]\n",
    "    start_time_str = f'20{time_str[:2]}-{time_str[2:4]}-{time_str[4:6]}_{time_str[6:8]}-{time_str[8:10]}-{time_str[10:12]}'\n",
    "    # Convert the start time to a datetime object\n",
    "    start_time = datetime.strptime(start_time_str, '%Y-%m-%d_%H-%M-%S')\n",
    "    time_step_size = 0.0005\n",
    "    \n",
    "    #SPACE INFO\n",
    "    # Set the window size\n",
    "    window_size = (400, 250)\n",
    "\n",
    "    # Set the overlap size\n",
    "    overlap_size = (300, 5)\n",
    "\n",
    "    # Calculate the stride as the window size minus the overlap size\n",
    "    stride = tuple(np.subtract(window_size, overlap_size))\n",
    "\n",
    "    # Calculate the distance location for each window\n",
    "    distance_location = np.arange(data.shape[0] - window_size[0] + 1, step=stride[0])\n",
    "    #print(distance_location)\n",
    "\n",
    "    \n",
    "    data_cut=data[300:,:]\n",
    "\n",
    "    [inputted,logits,window_start_time1,window_distance_location1]=sliding_window(data_cut,start_time,time_step_size,distance_location,stride) \n",
    "    apex=window_distance_location1+200\n",
    "    probability = tf.nn.sigmoid(logits).numpy()\n",
    "    positive_predictions=np.where(probability>0.5)\n",
    "    neg_predictions=np.where(probability<0.1)\n",
    "    #print(positive_predictions)\n",
    "    positive_windows=inputted[positive_predictions[0],positive_predictions[1],:,:]\n",
    "    neg_windows=inputted[neg_predictions[0],neg_predictions[1],:,:]\n",
    "    positive_prob=probability[positive_predictions[0],positive_predictions[1]]\n",
    "    positive_time=window_start_time1[positive_predictions[1]]\n",
    "    positive_dist=window_distance_location1[positive_predictions[0]]\n",
    "    np.save('/scratch/users/prgiven/continuous_data/continuous_processed/processed_data/FORGE2019_RESULTS/' + str(i)+'_pos_logits.npy',positive_prob)\n",
    "    np.save('/scratch/users/prgiven/continuous_data/continuous_processed/processed_data/FORGE2019_RESULTS/' + str(i)+'_pos_windows.npy',positive_windows)\n",
    "    np.save('/scratch/users/prgiven/continuous_data/continuous_processed/processed_data/FORGE2019_RESULTS/' + str(i)+'_pos_times.npy',positive_time)\n",
    "    np.save('/scratch/users/prgiven/continuous_data/continuous_processed/processed_data/FORGE2019_RESULTS/' + str(i)+'_pos_dist.npy',positive_dist)\n",
    "\n",
    "    probability_loop=np.append(probability_loop,probability)\n",
    "    time_loop=np.append(time_loop,window_start_time1)\n",
    "    ap_loop=np.append(ap_loop,apex)\n",
    "    print(filenames[i])\n",
    "    print(i)\n",
    "    #print(window_start_time)\n",
    "    print(\"Total Windows\",len(probability[probability>=0]))\n",
    "    print(\"Number of Positive Windows with <50% Probability\",len(probability[probability>0.5]))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c0b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,5))\n",
    "plt.hist(probability_loop,bins=50);\n",
    "plt.yscale('log')\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.xlim([0,1])\n",
    "plt.xlabel('Probability of Microseismic Event')\n",
    "plt.ylabel('Number of Windows')\n",
    "plt.title('Probability Distribution for FORGE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
